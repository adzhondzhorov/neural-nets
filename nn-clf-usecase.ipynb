{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305f180-0341-45a6-8372-29188ea2d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from lib.value import Value\n",
    "from lib.linear_algebra import Vector, Matrix\n",
    "from lib.nn import NN, Softmax, Linear\n",
    "from lib.processing import OneHotEncoder, ColumnNormalizer\n",
    "from lib.metrics.losses import negative_log_likelihood\n",
    "from lib.gd_data_loaders import BatchDataLoader, StochasticDataLoader, MiniBatchDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d99b45-d102-4f86-ba2b-3fc6b4732e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n",
    "# It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "with open(\"data/iris.data\", \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        data.append([float(v) for v in line.split(\",\")[:-1]])\n",
    "        labels.append(line.split(\",\")[-1])\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b375e393-c45b-4b80-9638-b8c733f6a361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces = list(range(len(data)))\n",
    "np.random.shuffle(indeces)\n",
    "split = int(len(data) * 0.8)\n",
    "\n",
    "X_train = data[indeces[:split]]\n",
    "X_test = data[indeces[split:]]\n",
    "y_train = [labels[i] for i in indeces[:split]]\n",
    "y_test = [labels[i] for i in indeces[split:]]\n",
    "X_train = Matrix(X_train)\n",
    "X_test = Matrix(X_test)\n",
    "X_train.dims(), X_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad713716-b457-4d51-932a-ba015a843d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 3), (30, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(labels)\n",
    "y_train = ohe.transform(y_train)\n",
    "y_test = ohe.transform(y_test)\n",
    "y_train.dims(), y_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4a5acf-6157-4196-a224-f50144fb6344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = ColumnNormalizer()\n",
    "normalizer.fit(X_train)\n",
    "X_train = normalizer.transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "X_train.dims(), X_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72cc6c4-edcc-40f2-a8fa-172eb5988a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN([\n",
    "    Linear(4, 3),\n",
    "    Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f66f69f-6592-45be-b963-3ae966069ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lib.gd_data_loaders.BatchDataLoader'>\n",
      "0 | 1.71 | 0s\n",
      "400 | 0.50 | 78s\n",
      "800 | 0.41 | 75s\n",
      "1200 | 0.35 | 72s\n",
      "1600 | 0.32 | 82s\n",
      "2000 | 0.29 | 76s\n",
      "2400 | 0.26 | 80s\n",
      "2800 | 0.24 | 84s\n",
      "3200 | 0.23 | 76s\n",
      "3600 | 0.21 | 78s\n",
      "4000 | 0.20 | 84s\n",
      "train loss: 0.20   test loss: 0.18\n",
      "<class 'lib.gd_data_loaders.StochasticDataLoader'>\n",
      "0 | 0.10 | 0s\n",
      "400 | 0.13 | 0s\n",
      "800 | 0.02 | 0s\n",
      "1200 | 0.06 | 0s\n",
      "1600 | 0.10 | 0s\n",
      "2000 | 0.01 | 0s\n",
      "2400 | 0.04 | 0s\n",
      "2800 | 0.08 | 0s\n",
      "3200 | 0.01 | 0s\n",
      "3600 | 0.03 | 0s\n",
      "4000 | 0.07 | 0s\n",
      "train loss: 0.14   test loss: 0.13\n",
      "<class 'lib.gd_data_loaders.MiniBatchDataLoader'>\n",
      "0 | 0.15 | 0s\n",
      "400 | 0.12 | 25s\n",
      "800 | 0.13 | 22s\n",
      "1200 | 0.12 | 22s\n",
      "1600 | 0.13 | 22s\n",
      "2000 | 0.13 | 19s\n",
      "2400 | 0.06 | 18s\n",
      "2800 | 0.13 | 19s\n",
      "3200 | 0.13 | 20s\n",
      "3600 | 0.08 | 21s\n",
      "4000 | 0.18 | 23s\n",
      "train loss: 0.11   test loss: 0.12\n"
     ]
    }
   ],
   "source": [
    "time_point = time.time()\n",
    "for data_loader in [\n",
    "    BatchDataLoader(X_train, y_train),\n",
    "    StochasticDataLoader(X_train, y_train),\n",
    "    MiniBatchDataLoader(X_train, y_train, 32)\n",
    "]:\n",
    "    print(data_loader.__class__)\n",
    "    for i in range(4001):\n",
    "        X_b, y_b = data_loader.get_batch()\n",
    "        out = nn(X_b)\n",
    "        loss = negative_log_likelihood(y_b, out)\n",
    "        \n",
    "        if i % 400 == 0:\n",
    "            elapsed_time = int(time.time() - time_point)\n",
    "            time_point = time.time()\n",
    "            print(f\"{i} | {loss.data:.2f} | {elapsed_time}s\")    \n",
    "    \n",
    "        for p in nn.params():\n",
    "            for v in p.all_values():\n",
    "                v.zero_grad()\n",
    "        loss.grad = 1\n",
    "        loss.backward()\n",
    "    \n",
    "        for p in nn.params():\n",
    "            for v in p.all_values():\n",
    "                v.data -= 0.01 * v.grad\n",
    "    \n",
    "    train_out = nn(X_train) \n",
    "    train_loss = negative_log_likelihood(y_train, train_out)\n",
    "    test_out = nn(X_test) \n",
    "    test_loss = negative_log_likelihood(y_test, test_out)\n",
    "    print(f\"train loss: {train_loss.data:.2f}   test loss: {test_loss.data:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35a35ec-a1ca-4b71-886d-28d92db4588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.1, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(round(float(v1),1), float(v2)) for v1, v2 in zip([v[0].data for v in test_out.values], [v[0].data for v in y_test])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
