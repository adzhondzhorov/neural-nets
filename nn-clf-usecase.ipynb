{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305f180-0341-45a6-8372-29188ea2d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from lib.value import Value\n",
    "from lib.linear_algebra import Vector, Matrix\n",
    "from lib.nn import NN, Softmax, Linear\n",
    "from lib.processing import OneHotEncoder, ColumnNormalizer\n",
    "from lib.metrics.losses import negative_log_likelihood\n",
    "from lib.gd_data_loaders import BatchDataLoader, StochasticDataLoader, MiniBatchDataLoader\n",
    "from lib.optimizers import SgdOptimizer, SgdWithMomentumOptimizer, AdaGradOptimizer, RmsPropOptimizer, AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d99b45-d102-4f86-ba2b-3fc6b4732e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n",
    "# It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "with open(\"data/iris.data\", \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        data.append([float(v) for v in line.split(\",\")[:-1]])\n",
    "        labels.append(line.split(\",\")[-1])\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b375e393-c45b-4b80-9638-b8c733f6a361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces = list(range(len(data)))\n",
    "np.random.shuffle(indeces)\n",
    "split = int(len(data) * 0.8)\n",
    "\n",
    "X_train = data[indeces[:split]]\n",
    "X_test = data[indeces[split:]]\n",
    "y_train = [labels[i] for i in indeces[:split]]\n",
    "y_test = [labels[i] for i in indeces[split:]]\n",
    "X_train = Matrix(X_train)\n",
    "X_test = Matrix(X_test)\n",
    "X_train.dims(), X_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad713716-b457-4d51-932a-ba015a843d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 3), (30, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(labels)\n",
    "y_train = ohe.transform(y_train)\n",
    "y_test = ohe.transform(y_test)\n",
    "y_train.dims(), y_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4a5acf-6157-4196-a224-f50144fb6344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = ColumnNormalizer()\n",
    "normalizer.fit(X_train)\n",
    "X_train = normalizer.transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "X_train.dims(), X_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72cc6c4-edcc-40f2-a8fa-172eb5988a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nn():\n",
    "    return NN([\n",
    "        Linear(4, 3),\n",
    "        Softmax()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f66f69f-6592-45be-b963-3ae966069ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent: <class 'lib.gd_data_loaders.BatchDataLoader'> | optimizer: <class 'lib.optimizers.SgdOptimizer'>\n",
      "0 | 1.71 | 0s\n",
      "400 | 0.50 | 71s\n",
      "800 | 0.41 | 72s\n",
      "1200 | 0.35 | 72s\n",
      "1600 | 0.32 | 72s\n",
      "2000 | 0.29 | 69s\n",
      "2400 | 0.26 | 67s\n",
      "2800 | 0.24 | 69s\n",
      "3200 | 0.23 | 76s\n",
      "3600 | 0.21 | 80s\n",
      "4000 | 0.20 | 72s\n",
      "train loss: 0.20   test loss: 0.18\n",
      "gradient descent: <class 'lib.gd_data_loaders.BatchDataLoader'> | optimizer: <class 'lib.optimizers.SgdWithMomentumOptimizer'>\n",
      "0 | 2.64 | 0s\n",
      "400 | 0.55 | 93s\n",
      "800 | 0.45 | 102s\n",
      "1200 | 0.38 | 88s\n",
      "1600 | 0.34 | 92s\n",
      "2000 | 0.30 | 94s\n",
      "2400 | 0.27 | 93s\n",
      "2800 | 0.25 | 107s\n",
      "3200 | 0.23 | 104s\n",
      "3600 | 0.22 | 106s\n",
      "4000 | 0.21 | 114s\n",
      "train loss: 0.21   test loss: 0.19\n",
      "gradient descent: <class 'lib.gd_data_loaders.BatchDataLoader'> | optimizer: <class 'lib.optimizers.AdaGradOptimizer'>\n",
      "0 | 2.73 | 0s\n",
      "400 | 0.23 | 113s\n",
      "800 | 0.16 | 88s\n",
      "1200 | 0.13 | 113s\n",
      "1600 | 0.11 | 112s\n",
      "2000 | 0.10 | 124s\n",
      "2400 | 0.09 | 136s\n",
      "2800 | 0.08 | 85s\n",
      "3200 | 0.08 | 81s\n",
      "3600 | 0.07 | 78s\n",
      "4000 | 0.07 | 90s\n",
      "train loss: 0.07   test loss: 0.10\n",
      "gradient descent: <class 'lib.gd_data_loaders.BatchDataLoader'> | optimizer: <class 'lib.optimizers.RmsPropOptimizer'>\n",
      "0 | 1.81 | 0s\n",
      "400 | 0.06 | 92s\n",
      "800 | 0.03 | 92s\n",
      "1200 | 0.03 | 98s\n",
      "1600 | 0.03 | 119s\n",
      "2000 | 0.03 | 122s\n",
      "2400 | 0.03 | 108s\n",
      "2800 | 0.03 | 87s\n",
      "3200 | 0.03 | 85s\n",
      "3600 | 0.03 | 85s\n",
      "4000 | 0.03 | 88s\n",
      "train loss: 0.03   test loss: 0.12\n",
      "gradient descent: <class 'lib.gd_data_loaders.BatchDataLoader'> | optimizer: <class 'lib.optimizers.AdamOptimizer'>\n",
      "0 | 2.66 | 0s\n",
      "400 | 0.07 | 86s\n",
      "800 | 0.03 | 84s\n",
      "1200 | 0.03 | 87s\n",
      "1600 | 0.03 | 86s\n",
      "2000 | 0.03 | 91s\n",
      "2400 | 0.03 | 86s\n",
      "2800 | 0.03 | 85s\n",
      "3200 | 0.03 | 85s\n",
      "3600 | 0.03 | 87s\n",
      "4000 | 0.03 | 105s\n",
      "train loss: 0.03   test loss: 0.12\n",
      "gradient descent: <class 'lib.gd_data_loaders.StochasticDataLoader'> | optimizer: <class 'lib.optimizers.SgdOptimizer'>\n",
      "0 | 4.19 | 0s\n",
      "400 | 0.30 | 0s\n",
      "800 | 0.06 | 0s\n",
      "1200 | 0.57 | 0s\n",
      "1600 | 0.17 | 0s\n",
      "2000 | 0.03 | 0s\n",
      "2400 | 0.21 | 0s\n",
      "2800 | 0.14 | 0s\n",
      "3200 | 0.02 | 0s\n",
      "3600 | 0.09 | 0s\n",
      "4000 | 0.11 | 0s\n",
      "train loss: 0.20   test loss: 0.18\n",
      "gradient descent: <class 'lib.gd_data_loaders.StochasticDataLoader'> | optimizer: <class 'lib.optimizers.SgdWithMomentumOptimizer'>\n",
      "0 | 2.85 | 0s\n",
      "400 | 0.86 | 0s\n",
      "800 | 0.58 | 0s\n",
      "1200 | 0.04 | 0s\n",
      "1600 | 0.47 | 0s\n",
      "2000 | 0.21 | 0s\n",
      "2400 | 0.03 | 0s\n",
      "2800 | 0.34 | 0s\n",
      "3200 | 0.13 | 0s\n",
      "3600 | 0.02 | 0s\n",
      "4000 | 0.26 | 0s\n",
      "train loss: 0.20   test loss: 0.19\n",
      "gradient descent: <class 'lib.gd_data_loaders.StochasticDataLoader'> | optimizer: <class 'lib.optimizers.AdaGradOptimizer'>\n",
      "0 | 2.25 | 0s\n",
      "400 | 0.01 | 0s\n",
      "800 | 0.20 | 0s\n",
      "1200 | 0.02 | 0s\n",
      "1600 | 0.00 | 0s\n",
      "2000 | 0.11 | 0s\n",
      "2400 | 0.01 | 0s\n",
      "2800 | 0.00 | 0s\n",
      "3200 | 0.08 | 0s\n",
      "3600 | 0.01 | 1s\n",
      "4000 | 0.00 | 0s\n",
      "train loss: 0.17   test loss: 0.17\n",
      "gradient descent: <class 'lib.gd_data_loaders.StochasticDataLoader'> | optimizer: <class 'lib.optimizers.RmsPropOptimizer'>\n",
      "0 | 0.25 | 0s\n",
      "400 | 0.01 | 0s\n",
      "800 | 0.05 | 0s\n",
      "1200 | 0.04 | 0s\n",
      "1600 | 0.00 | 0s\n",
      "2000 | 0.01 | 0s\n",
      "2400 | 0.02 | 0s\n",
      "2800 | 0.00 | 0s\n",
      "3200 | 0.00 | 0s\n",
      "3600 | 0.01 | 0s\n",
      "4000 | 0.00 | 0s\n",
      "train loss: 0.06   test loss: 0.10\n",
      "gradient descent: <class 'lib.gd_data_loaders.StochasticDataLoader'> | optimizer: <class 'lib.optimizers.AdamOptimizer'>\n",
      "0 | 4.10 | 0s\n",
      "400 | 0.04 | 0s\n",
      "800 | 0.00 | 0s\n",
      "1200 | 0.00 | 0s\n",
      "1600 | 0.01 | 0s\n",
      "2000 | 0.00 | 0s\n",
      "2400 | 0.00 | 0s\n",
      "2800 | 0.00 | 0s\n",
      "3200 | 0.00 | 0s\n",
      "3600 | 0.00 | 0s\n",
      "4000 | 0.00 | 0s\n",
      "train loss: 0.06   test loss: 0.11\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.SgdOptimizer'>\n",
      "0 | 0.73 | 0s\n",
      "400 | 0.37 | 24s\n",
      "800 | 0.42 | 22s\n",
      "1200 | 0.35 | 24s\n",
      "1600 | 0.28 | 26s\n",
      "2000 | 0.23 | 23s\n",
      "2400 | 0.23 | 24s\n",
      "2800 | 0.22 | 27s\n",
      "3200 | 0.20 | 31s\n",
      "3600 | 0.16 | 25s\n",
      "4000 | 0.17 | 26s\n",
      "train loss: 0.19   test loss: 0.18\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.SgdWithMomentumOptimizer'>\n",
      "0 | 2.24 | 0s\n",
      "400 | 0.38 | 23s\n",
      "800 | 0.36 | 24s\n",
      "1200 | 0.14 | 30s\n",
      "1600 | 0.28 | 28s\n",
      "2000 | 0.27 | 30s\n",
      "2400 | 0.18 | 24s\n",
      "2800 | 0.22 | 31s\n",
      "3200 | 0.20 | 31s\n",
      "3600 | 0.23 | 35s\n",
      "4000 | 0.16 | 37s\n",
      "train loss: 0.18   test loss: 0.17\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.AdaGradOptimizer'>\n",
      "0 | 0.46 | 0s\n",
      "400 | 0.05 | 34s\n",
      "800 | 0.10 | 37s\n",
      "1200 | 0.07 | 20s\n",
      "1600 | 0.06 | 21s\n",
      "2000 | 0.04 | 24s\n",
      "2400 | 0.10 | 26s\n",
      "2800 | 0.06 | 27s\n",
      "3200 | 0.02 | 25s\n",
      "3600 | 0.02 | 34s\n",
      "4000 | 0.03 | 26s\n",
      "train loss: 0.05   test loss: 0.10\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.RmsPropOptimizer'>\n",
      "0 | 1.55 | 0s\n",
      "400 | 0.09 | 25s\n",
      "800 | 0.03 | 27s\n",
      "1200 | 0.06 | 26s\n",
      "1600 | 0.08 | 31s\n",
      "2000 | 0.04 | 28s\n",
      "2400 | 0.02 | 36s\n",
      "2800 | 0.02 | 25s\n",
      "3200 | 0.09 | 25s\n",
      "3600 | 0.09 | 28s\n",
      "4000 | 0.07 | 27s\n",
      "train loss: 0.04   test loss: 0.12\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.AdamOptimizer'>\n",
      "0 | 2.60 | 0s\n",
      "400 | 0.13 | 28s\n",
      "800 | 0.02 | 28s\n",
      "1200 | 0.02 | 40s\n",
      "1600 | 0.02 | 26s\n",
      "2000 | 0.08 | 27s\n",
      "2400 | 0.04 | 25s\n",
      "2800 | 0.03 | 25s\n",
      "3200 | 0.01 | 31s\n",
      "3600 | 0.01 | 33s\n",
      "4000 | 0.01 | 24s\n",
      "train loss: 0.04   test loss: 0.12\n"
     ]
    }
   ],
   "source": [
    "time_point = time.time()\n",
    "\n",
    "data_loaders = [\n",
    "    BatchDataLoader(X_train, y_train),\n",
    "    StochasticDataLoader(X_train, y_train),\n",
    "    MiniBatchDataLoader(X_train, y_train, 32)\n",
    "]\n",
    "optimizer_creators = [\n",
    "    lambda nn: SgdOptimizer(nn, 0.01),\n",
    "    lambda nn: SgdWithMomentumOptimizer(nn, 0.01, 0.9),\n",
    "    lambda nn: AdaGradOptimizer(nn, 0.1),\n",
    "    lambda nn: RmsPropOptimizer(nn, 0.01, 0.95),\n",
    "    lambda nn: AdamOptimizer(nn, 0.01, 0.95, 0.95),\n",
    "]\n",
    "\n",
    "for data_loader in data_loaders:\n",
    "    for optimizer_creator in optimizer_creators:\n",
    "        nn = init_nn()\n",
    "        optimizer = optimizer_creator(nn)\n",
    "        print(f\"gradient descent: {data_loader.__class__} | optimizer: {optimizer.__class__}\")\n",
    "        for i in range(4001):\n",
    "            X_b, y_b = data_loader.get_batch()\n",
    "            out = nn(X_b)\n",
    "            loss = negative_log_likelihood(y_b, out)\n",
    "            \n",
    "            if i % 400 == 0:\n",
    "                elapsed_time = int(time.time() - time_point)\n",
    "                time_point = time.time()\n",
    "                print(f\"{i} | {loss.data:.2f} | {elapsed_time}s\")    \n",
    "    \n",
    "            optimizer.step(loss)\n",
    "        \n",
    "        train_out = nn(X_train) \n",
    "        train_loss = negative_log_likelihood(y_train, train_out)\n",
    "        test_out = nn(X_test) \n",
    "        test_loss = negative_log_likelihood(y_test, test_out)\n",
    "        print(f\"train loss: {train_loss.data:.2f}   test loss: {test_loss.data:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35a35ec-a1ca-4b71-886d-28d92db4588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(round(float(v1),1), float(v2)) for v1, v2 in zip([v[0].data for v in test_out.values], [v[0].data for v in y_test])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
