{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a54d313-d375-400b-b11a-aa88d25b2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"pt_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d305f180-0341-45a6-8372-29188ea2d7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/local/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/f8/ytjnv4ks6rq7m9lf3hh7_60r0000gn/T/ipykernel_24833/1990074465.py\", line 23, in <module>\n",
      "    from lib.pt_backend.linear_algebra import Matrix\n",
      "  File \"/Users/anton/repos/neural-nets/lib/pt_backend/linear_algebra.py\", line 2, in <module>\n",
      "    from torch import Tensor, log\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/anton/repos/neural-nets/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "match backend:\n",
    "    case \"original_backend\":\n",
    "        from lib.original_backend.linear_algebra import Matrix\n",
    "        from lib.original_backend.nn import  NN, Softmax, Linear\n",
    "        from lib.original_backend.processing import OneHotEncoder, ColumnNormalizer\n",
    "        from lib.optimizers import SgdOptimizer, SgdWithMomentumOptimizer, AdaGradOptimizer, RmsPropOptimizer, AdamOptimizer\n",
    "\n",
    "    case \"np_backend\":\n",
    "        from lib.np_backend.linear_algebra import Matrix\n",
    "        from lib.np_backend.nn import NN, Softmax, Linear\n",
    "        from lib.np_backend.processing import OneHotEncoder, ColumnNormalizer\n",
    "        from lib.optimizers import SgdOptimizer, SgdWithMomentumOptimizer, AdaGradOptimizer, RmsPropOptimizer, AdamOptimizer\n",
    "\n",
    "    case \"pt_backend\":\n",
    "        from lib.pt_backend.linear_algebra import Matrix\n",
    "        from lib.pt_backend.nn import NN, Softmax, Linear\n",
    "        from lib.pt_backend.processing import OneHotEncoder, ColumnNormalizer\n",
    "        from lib.pt_backend.optimizers import SgdOptimizer, SgdWithMomentumOptimizer, AdaGradOptimizer, RmsPropOptimizer, AdamOptimizer\n",
    "\n",
    "\n",
    "from lib.metrics.losses import negative_log_likelihood\n",
    "from lib.gd_data_loaders import BatchDataLoader, StochasticDataLoader, MiniBatchDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d99b45-d102-4f86-ba2b-3fc6b4732e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n",
    "# It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "with open(\"data/iris.data\", \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        data.append([float(v) for v in line.split(\",\")[:-1]])\n",
    "        labels.append(line.split(\",\")[-1])\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b375e393-c45b-4b80-9638-b8c733f6a361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 4]), torch.Size([30, 4]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces = list(range(len(data)))\n",
    "np.random.shuffle(indeces)\n",
    "split = int(len(data) * 0.8)\n",
    "\n",
    "X_train = data[indeces[:split]]\n",
    "X_test = data[indeces[split:]]\n",
    "y_train = [labels[i] for i in indeces[:split]]\n",
    "y_test = [labels[i] for i in indeces[split:]]\n",
    "X_train = Matrix(X_train)\n",
    "X_test = Matrix(X_test)\n",
    "X_train.dims(), X_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad713716-b457-4d51-932a-ba015a843d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 3]), torch.Size([30, 3]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(labels)\n",
    "y_train = ohe.transform(y_train)\n",
    "y_test = ohe.transform(y_test)\n",
    "y_train.dims(), y_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4a5acf-6157-4196-a224-f50144fb6344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 4]), torch.Size([30, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = ColumnNormalizer()\n",
    "normalizer.fit(X_train)\n",
    "X_train = normalizer.transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "X_train.dims(), X_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72cc6c4-edcc-40f2-a8fa-172eb5988a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nn():\n",
    "    return NN([\n",
    "        Linear(4, 3),\n",
    "        Softmax()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f66f69f-6592-45be-b963-3ae966069ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.pt_backend.optimizers.SgdOptimizer'>\n",
      "0 | 0.78 | 0s\n",
      "400 | 0.44 | 0s\n",
      "800 | 0.08 | 0s\n",
      "1200 | 0.09 | 0s\n",
      "1600 | 0.33 | 0s\n",
      "2000 | 0.33 | 0s\n",
      "2400 | 0.36 | 0s\n",
      "2800 | 0.27 | 0s\n",
      "3200 | 0.26 | 0s\n",
      "3600 | 0.13 | 0s\n",
      "4000 | 0.08 | 0s\n",
      "train loss: 0.19   test loss: 0.18\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.pt_backend.optimizers.SgdWithMomentumOptimizer'>\n",
      "0 | 0.98 | 0s\n",
      "400 | 0.31 | 0s\n",
      "800 | 0.34 | 0s\n",
      "1200 | 0.22 | 0s\n",
      "1600 | 0.23 | 0s\n",
      "2000 | 0.13 | 0s\n",
      "2400 | 0.11 | 0s\n",
      "2800 | 0.09 | 0s\n",
      "3200 | 0.13 | 0s\n",
      "3600 | 0.17 | 0s\n",
      "4000 | 0.16 | 0s\n",
      "train loss: 0.19   test loss: 0.18\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.pt_backend.optimizers.AdaGradOptimizer'>\n",
      "0 | 0.77 | 0s\n",
      "400 | 0.35 | 0s\n",
      "800 | 0.19 | 0s\n",
      "1200 | 0.10 | 0s\n",
      "1600 | 0.02 | 0s\n",
      "2000 | 0.01 | 0s\n",
      "2400 | 0.19 | 0s\n",
      "2800 | 0.00 | 0s\n",
      "3200 | 0.13 | 0s\n",
      "3600 | 0.08 | 0s\n",
      "4000 | 0.02 | 0s\n",
      "train loss: 0.10   test loss: 0.12\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.pt_backend.optimizers.RmsPropOptimizer'>\n",
      "0 | 1.10 | 0s\n",
      "400 | 0.05 | 0s\n",
      "800 | 0.02 | 0s\n",
      "1200 | 0.02 | 0s\n",
      "1600 | 0.08 | 0s\n",
      "2000 | 0.04 | 0s\n",
      "2400 | 0.03 | 0s\n",
      "2800 | 0.04 | 0s\n",
      "3200 | 0.05 | 0s\n",
      "3600 | 0.02 | 0s\n",
      "4000 | 0.12 | 0s\n",
      "train loss: 0.04   test loss: 0.11\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.pt_backend.optimizers.AdamOptimizer'>\n",
      "0 | 0.83 | 0s\n",
      "400 | 0.14 | 0s\n",
      "800 | 0.15 | 0s\n",
      "1200 | 0.02 | 0s\n",
      "1600 | 0.07 | 0s\n",
      "2000 | 0.22 | 0s\n",
      "2400 | 0.05 | 0s\n",
      "2800 | 0.00 | 0s\n",
      "3200 | 0.00 | 0s\n",
      "3600 | 0.05 | 0s\n",
      "4000 | 0.03 | 0s\n",
      "train loss: 0.04   test loss: 0.10\n"
     ]
    }
   ],
   "source": [
    "time_point = time.time()\n",
    "\n",
    "data_loaders = [\n",
    "    # BatchDataLoader(X_train, y_train),\n",
    "    # StochasticDataLoader(X_train, y_train),\n",
    "    MiniBatchDataLoader(X_train, y_train, 4)\n",
    "]\n",
    "optimizer_creators = [\n",
    "    lambda nn: SgdOptimizer(nn, 0.01),\n",
    "    lambda nn: SgdWithMomentumOptimizer(nn, 0.01, 0.9),\n",
    "    lambda nn: AdaGradOptimizer(nn, 0.1),\n",
    "    lambda nn: RmsPropOptimizer(nn, 0.01, 0.95),\n",
    "    lambda nn: AdamOptimizer(nn, 0.01, 0.95, 0.95),\n",
    "]\n",
    "\n",
    "for data_loader in data_loaders:\n",
    "    for optimizer_creator in optimizer_creators:\n",
    "        nn = init_nn()\n",
    "        optimizer = optimizer_creator(nn)\n",
    "        print(f\"gradient descent: {data_loader.__class__} | optimizer: {optimizer.__class__}\")\n",
    "        for i in range(4001):\n",
    "            X_b, y_b = data_loader.get_batch()\n",
    "            out = nn(X_b)\n",
    "            loss = negative_log_likelihood(y_b, out)\n",
    "            \n",
    "            if i % 400 == 0:\n",
    "                elapsed_time = int(time.time() - time_point)\n",
    "                time_point = time.time()\n",
    "                print(f\"{i} | {loss.data:.2f} | {elapsed_time}s\")    \n",
    "    \n",
    "            optimizer.step(loss)\n",
    "        \n",
    "        train_out = nn(X_train) \n",
    "        train_loss = negative_log_likelihood(y_train, train_out)\n",
    "        test_out = nn(X_test) \n",
    "        test_loss = negative_log_likelihood(y_test, test_out)\n",
    "        print(f\"train loss: {train_loss.data:.2f}   test loss: {test_loss.data:.2f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
