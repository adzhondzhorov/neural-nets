{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc67fd52-202c-4061-82cf-a0e810734311",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ce138a-ac1b-417a-8fc4-4a2eec1f4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "if USE_FAST:\n",
    "    from lib.fast.linear_algebra import Matrix\n",
    "    from lib.fast.nn import Linear, ReLU, NN, Sigmoid, Embedding, Flatten, MaxPooling\n",
    "else:\n",
    "    from lib.original.linear_algebra import Matrix\n",
    "    from lib.original.nn import Linear, ReLU, NN, Sigmoid, Embedding, Flatten, MaxPooling\n",
    "from lib.tokenization import BPETokenizer\n",
    "from lib.io import load_tokenizer, save_tokenizer\n",
    "from lib.metrics.losses import binary_cross_entropy\n",
    "from lib.metrics.evaluations import accuracy\n",
    "from lib.gd_data_loaders import BatchDataLoader, StochasticDataLoader, MiniBatchDataLoader\n",
    "from lib.optimizers import SgdOptimizer, SgdWithMomentumOptimizer, AdaGradOptimizer, RmsPropOptimizer, AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb8375b-2e86-4d7c-ad96-15113163f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "# This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing.\n",
    "tokenizer_data = []\n",
    "read_header = False\n",
    "assert len(\"negative\") == len(\"positive\")\n",
    "label_len = len(\"negative\") + 1\n",
    "with open(\"data/imdb_dataset.csv\", \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if not read_header:\n",
    "            read_header = True\n",
    "            continue\n",
    "        tokenizer_data.append([line[:-label_len - 1], line[-label_len:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7da1f93-0d13-4e1b-9459-60feeef17014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = BPETokenizer(3000)\n",
    "# tokenizer.fit([d[0] for d in tokenizer_data])\n",
    "# save_tokenizer(tokenizer, \"imdb_tokenizer_3K\")\n",
    "\n",
    "tokenizer = load_tokenizer(\"imdb_tokenizer_3K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7332f2a-82be-4545-8d4d-316dc7a15648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipkart Customer Review and Rating\n",
    "# This Dataset consists of reviews by customers on boAt Rockerz 400\n",
    "\n",
    "data = []\n",
    "read_header = False\n",
    "with open(\"data/flipkart_reviews.csv\", \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if not read_header:\n",
    "            read_header = True\n",
    "            continue\n",
    "        data.append([line[:-12], int(int(line[-2])>=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd0cfaa-4a73-488b-989a-d4eb3dceeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d for d in data if d[1]==1][:500] + [d for d in data if d[1]==0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e01fbbf-b20f-44a5-8498-e7a95fb69eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    tokens_list = pool.map(tokenizer.encode, [d[0] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd5569a-8ea3-45d6-b0d6-6e22ce5a445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d[1] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d99904f-b5a2-4ce3-bc48-60665031f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "for tokens in tokens_list:\n",
    "    del tokens[max_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4285d423-e281-4814-a767-22afb3d04dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ill_formatted_indices = [] \n",
    "for i, tokens in enumerate(tokens_list):\n",
    "    if any(isinstance(t, str) for t in tokens):\n",
    "        ill_formatted_indices.append(i)\n",
    "\n",
    "tokens_list = [t for i, t in enumerate(tokens_list) if i not in ill_formatted_indices]\n",
    "labels = [l for i, l in enumerate(labels) if i not in ill_formatted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdca7a57-1ec3-499e-8bd5-2ab2908e1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(tokens) for tokens in tokens_list])\n",
    "for tokens in tokens_list:\n",
    "    tokens.extend([0] * (max_len - len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6220432-c0ac-4ef1-9cdd-a7601ed81476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((771, 32), (771, 1), (193, 32), (193, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = random.sample(range(len(tokens_list)), int(len(tokens_list) * 0.8))\n",
    "X_train, y_train = Matrix([t for i, t in enumerate(tokens_list) if i in indices]), Matrix([[l] for i, l in enumerate(labels) if i in indices])\n",
    "X_test, y_test = Matrix([t for i, t in enumerate(tokens_list) if i not in indices]), Matrix([[l] for i, l in enumerate(labels) if i not in indices])\n",
    "X_train.dims(), y_train.dims(), X_test.dims(), y_test.dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21720a5e-9e9c-4c7c-a582-dcd8aa3ef477",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.vocab)\n",
    "emb_size = 8\n",
    "\n",
    "size_dim, seq_len = X_train.dims()\n",
    "\n",
    "def init_nn():\n",
    "    return NN(\n",
    "        [\n",
    "            Embedding(vocab_size, emb_size),\n",
    "            Flatten(),\n",
    "            Linear(seq_len * emb_size, 16, \"uniform_glorot\"),\n",
    "            ReLU(),\n",
    "            Linear(16, 16, \"uniform_glorot\"),\n",
    "            ReLU(),\n",
    "            Linear(16, 1, \"uniform_glorot\"),\n",
    "            Sigmoid(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70f207b-0efe-43e7-ad7f-555d19a6624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.RmsPropOptimizer'>\n",
      "0 | 0.78 | 1s\n",
      "1 | 0.73 | 3s\n",
      "2 | 0.67 | 3s\n",
      "3 | 0.78 | 3s\n",
      "4 | 0.74 | 4s\n",
      "5 | 0.62 | 3s\n",
      "6 | 0.64 | 3s\n",
      "7 | 0.66 | 3s\n",
      "8 | 0.61 | 4s\n",
      "9 | 0.73 | 3s\n",
      "train loss: 0.63   test loss: 0.66   test accuracy: 0.58\n",
      "gradient descent: <class 'lib.gd_data_loaders.MiniBatchDataLoader'> | optimizer: <class 'lib.optimizers.AdamOptimizer'>\n",
      "0 | 0.86 | 190s\n",
      "1 | 0.89 | 1s\n",
      "2 | 0.84 | 1s\n",
      "3 | 0.92 | 1s\n",
      "4 | 0.76 | 1s\n",
      "5 | 0.74 | 1s\n",
      "6 | 0.89 | 1s\n",
      "7 | 0.74 | 1s\n",
      "8 | 0.73 | 1s\n",
      "9 | 0.74 | 1s\n",
      "train loss: 0.64   test loss: 0.69   test accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 1e-5\n",
    "time_point = time.time()\n",
    "\n",
    "data_loaders = [\n",
    "    # BatchDataLoader(X_train, y_train),\n",
    "    # StochasticDataLoader(X_train, y_train),\n",
    "    MiniBatchDataLoader(X_train, y_train, 16)\n",
    "]\n",
    "optimizer_creators = [\n",
    "    # lambda nn: SgdOptimizer(nn, 0.001),\n",
    "    # lambda nn: SgdWithMomentumOptimizer(nn, 0.001, 0.9),\n",
    "    # lambda nn: AdaGradOptimizer(nn, 0.001),\n",
    "    lambda nn: RmsPropOptimizer(nn, 0.001, 0.95),\n",
    "    lambda nn: AdamOptimizer(nn, 0.005, 0.95, 0.99),\n",
    "]\n",
    "\n",
    "for data_loader in data_loaders:\n",
    "    for optimizer_creator in optimizer_creators:\n",
    "        nn = init_nn()\n",
    "        optimizer = optimizer_creator(nn)\n",
    "        print(f\"gradient descent: {data_loader.__class__} | optimizer: {optimizer.__class__}\")\n",
    "        for i in range(10):\n",
    "            X_b, y_b = data_loader.get_batch()\n",
    "            out = nn(X_b)\n",
    "            loss = binary_cross_entropy(y_b, out)\n",
    "        \n",
    "            elapsed_time = int(time.time() - time_point)\n",
    "            print(f\"{i} | {loss.data:.2f} | {elapsed_time}s\")\n",
    "            time_point = time.time()\n",
    "\n",
    "            optimizer.step(loss)\n",
    "                \n",
    "        train_out = nn(X_train) \n",
    "        train_loss = binary_cross_entropy(y_train, train_out)\n",
    "        test_out = nn(X_test) \n",
    "        test_loss = binary_cross_entropy(y_test, test_out)\n",
    "        test_acc = accuracy(y_test, test_out)\n",
    "        print(f\"train loss: {train_loss.data:.2f}   test loss: {test_loss.data:.2f}   test accuracy: {test_acc:.2f}\") \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9f95202-c527-48bb-a615-447b7bbc860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great looks n very light weight sound is very cool but design is ok ok. t',\n",
       "  0.62,\n",
       "  1.0),\n",
       " ('light weight and easy to carry anywhere and attractive model awesome service from flip',\n",
       "  0.71,\n",
       "  1.0),\n",
       " ('\"The boat product is good excellent headphone to buy with Bluetooth without ',\n",
       "  0.29,\n",
       "  1.0),\n",
       " ('Product is good battery backup is also good. I recommend you should bu',\n",
       "  0.48,\n",
       "  1.0),\n",
       " ('\"Firstly, Thanks to Flipkart...I got the headphones 4 days earlier...',\n",
       "  0.8,\n",
       "  1.0),\n",
       " ('\"Got this headphone at just rs. 899 in offer i can say it\\'s just a',\n",
       "  0.46,\n",
       "  1.0),\n",
       " ('\"Battery Backup Is \"\"Ashvatthama\"\" Which Just Denies To Die , St',\n",
       "  0.27,\n",
       "  1.0),\n",
       " ('sound quality good enough considering that I got it for 1000. some might find ',\n",
       "  0.46,\n",
       "  1.0),\n",
       " ('Excellent jst awesome product... Love it at 999 it is best<SEPARATOR><SEPARATOR><SEPARATOR><SEPARATOR><SEPARATOR><SEPARATOR>',\n",
       "  0.33,\n",
       "  1.0),\n",
       " ('awesome product crystal clear sound with superb bass. I buy these for RS 999 ',\n",
       "  0.51,\n",
       "  1.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [(tokenizer.decode([int(t.data) for t in xt]), round(o[0].data, 2), yt[0].data) for o, yt, xt in zip(out, y_test, X_test)]\n",
    "examples[:5] + examples[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff15e0-dc84-4571-ac22-1bb8c47c2843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
